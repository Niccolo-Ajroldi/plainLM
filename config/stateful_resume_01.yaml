## CONFIG file
## change paths with your own!

deterministic: False
seed: 100

trainset_path: "/fast/najroldi/data/lm/fwedu/fwedu_sample_10B_tokenizer_GPT2/ctx_2048/train" # tokens: 9_849_288_704
vocab_size: 50257
seq_len: 2048
intra_doc_masking: False
sampler: 'stateful_random'
sampler_seed: 1996
num_workers: 4

eval: True
validset_path: "/fast/najroldi/data/lm/fwedu/fwedu_sample_10B_tokenizer_GPT2/ctx_2048/train"
valid_tokens: 1_000_000
eval_every_steps: 100

model: 'transformer'
d_model: 768
mlp_class: 'glu'
expand: '8/3'
n_layers: 12
n_heads: 12
rms_norm: True
tie_embeddings: True
torch_compile: True

# note: step budget=token_budget/(seq_len * micro_batch_size * grad_accumulation_steps * ddp_world_size)
steps_budget: 18500

# note: this is micro batch size if grad_accumulation_steps>1
# note: with ddp, effective batch size = batch_size * grad_accumulation_steps * ddp_world_size
micro_batch_size: 32
grad_accumulation_steps: 2

# note: choose between {float32, float16, bfloat16}
# note: float16 data type will automatically use a GradScaler
dtype: 'bfloat16'

optim: 'adamw'
fused_optim: True
lr: 3.e-3
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
grad_clip: 1.0

scheduler: 'linear_cooldown'
warmup_steps: 0.01
cooldown_steps: 3500
lr_start: 0.0
lr_end: 0.0
lr_end_pct: null

log_every_steps: 50
print_progress: True

use_wandb: True
wandb_project: 'doc_mask'
wandb_dir: '/fast/najroldi/logs/doc_mask/wandb'
wandb_run_name: '130M_cooldown_01'
exp_name: '130M_cooldown_01'
out_dir: '/fast/najroldi/exp/doc_mask'
over_write: True

resume: True
resume_exp_name: "/fast/najroldi/exp/doc_mask/130M_save_01/job_idx_1"
resume_step: [
  500,
  1500,
  2500,
  3500,
  4500,
  5500,
  6500,
  7500,
  8500,
  9500,
  10500,
  11500,
  12500,
  13500,
  14500,
] # TOT: 15

save_last_checkpoint: True
save_intermediate_checkpoints: True
save_every_steps: 500

