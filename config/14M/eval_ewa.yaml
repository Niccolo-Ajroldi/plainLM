
deterministic: True
seed: 1996

# avg
eval_ckpt_folder: "/fast/najroldi/exp/llm/plm_14M_save"
avg_start_step: 10
avg_every_steps: 2
avg_scheme: "EWA"
# ewa_beta: 0.99
ewa_beta: [0.0, 0.9, 0.99]

cuda_matmul_allow_tf32: False
cudnn_allow_tf32: False

trainset_path: "/fast/najroldi/data/lm/fineweb/fw_20B_tokens_ctx1024/train"
vocab_size: 50257
seq_len: 1024
num_workers: 4

sampler: 'stateful_random'
sampler_seed: 1996

eval: True
validset_path: "/fast/najroldi/data/lm/fineweb/fw_20B_tokens_ctx1024/valid"
eval_every_steps: 10

model: 'transformer'
d_model: 128
expand: '8/3'
n_layers: 4
n_heads: 4
mlp_class: 'glu'
tie_embeddings: True
torch_compile: False

# note: step budget=token_budget/(seq_len * micro_batch_size * grad_accumulation_steps)
steps_budget: 100

# note 1: this is micro batch size if grad_accumulation_steps>1
# note 2: with ddp, effective batch size = batch_size * grad_accumulation_steps * ddp_world_size
micro_batch_size: 8
grad_accumulation_steps: 1

# note: choose between {float32, float16, bfloat16}
# note: float16 data type will automatically use a GradScaler
dtype: 'bfloat16'

optim: 'adamw'
fused_optim: True
lr: 0.001
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
grad_clip: 1.0
eps: 1.e-20

scheduler: 'warmup_cosine'
warmup_steps: 20
cooldown_steps: null
lr_start: 1.e-10
lr_end: null
lr_end_pct: 0.001

log_every_steps: 10
print_progress: False

use_wandb: True
wandb_watch: False
wandb_project: 'plainLM_tests'
wandb_dir: '/fast/najroldi/logs/wa_lm/wandb'
check_existing_wandb_run: False
wandb_run_name: 'plm_14M_EMA'
exp_name: 'plm_14M_EMA'

resume: False
resume_step: null
resume_exp_name: null

out_dir: '/fast/najroldi/exp/llm'
save_last_checkpoint: False
save_intermediate_checkpoints: False
save_every_steps: 10
over_write: True
