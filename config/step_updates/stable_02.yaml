# trainset: 99_762_225_152 (~100BT)
# validset:      9_994_240 (~10MT)

deterministic: False
seed: 100

trainset_path: "/fast/najroldi/data/lm/fwedu/fwedu_sample_100BT/tokenized_EleutherAI_gpt-neox-20b/ctx_2048/train"
vocab_size: 50277
seq_len: 2048
intra_doc_masking: False
sampler: 'stateful_random'
sampler_seed: 1996
num_workers: 4

eval: True
validset_path: "/fast/najroldi/data/lm/fwedu/fwedu_sample_100BT/tokenized_EleutherAI_gpt-neox-20b/ctx_2048/valid"
valid_tokens: 1000000  # 1M
eval_every_steps: 1000

model: 'pythia-160m'
torch_compile: True

# Allow TF32 matmul and cudnn
cuda_matmul_allow_tf32: True
cudnn_allow_tf32: True

# note: step budget=token_budget/(seq_len * micro_batch_size * grad_accumulation_steps * ddp_world_size)
# steps_budget: 190000 # 99_614_720_000 tokens
steps_budget: 152500 # Do only 500 steps for science

# note: this is micro batch size if grad_accumulation_steps>1
# note: with ddp, effective batch size = batch_size * grad_accumulation_steps * ddp_world_size
micro_batch_size:  8
grad_accumulation_steps: 16

# note: choose between {float32, float16, bfloat16}
# note: float16 data type will automatically use a GradScaler
dtype: 'bfloat16'

optim: 'custom_adam'
fused_optim: True
lr: 1.e-3
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
grad_clip: 1.0
corrected_weight_decay: False
log_grad_norms: False

scheduler: ['wsd']
warmup_steps: 0.01
cooldown_steps: 0 # no cooldown this experiment studies the ste update during the stable phase
lr_start: 0.0
lr_end: 0.0
lr_end_pct: null

log_every_steps: 5
print_progress: True

use_wandb: True
wandb_project: 'quantization_pretraining'
wandb_dir: '/fast/atatjer/wandb/quantization_pretraining/'
wandb_run_name: 'step_updates_500steps_stable_02'
exp_name: 'step_updates_500steps_stable_02'
out_dir: '/fast/atatjer/scalinglawsquantization/checkpoints'
over_write: True

resume: True
resume_step: 152000
resume_exp_name: /fast/atatjer/scalinglawsquantization/checkpoints/pythia_160M_100BT_baseline_04/job_idx_1/

save_last_checkpoint: True
save_intermediate_checkpoints: True
save_every_steps: 5 # save the model every X steps
save_optim_every_steps: 5 # save optimizer every Y steps
save_step_update_every_steps: 5 # save optimizer every Y steps
